import Pkg
Pkg.add("Optim")

using Optim
using DelimitedFiles
using LinearAlgebra
using Distributions


A=readdlm("/Users/bubbles/Desktop/751 Labor/751 Chao/Empirical Project/labor.txt", Float64)


N=1506
T_data=10
K=43  #the max initial experience is 24, so the max exp at t=19 is 24+19

b_in=1000  #fixed cost of working
#parameters in Mincer earnings function
β1_in=9.93
β2_in=0.01
β3_in=-0.0002
β4_in=0.035
σ_in=1 #standard deviation of ε
σe_in=2  #standard deviation of measurement error
σu_in=sqrt(σ_in^2+σe_in^2)

#parameters in the utility function
α1_in=-14500
α2_in=-0.25
α3_in=-100
α4_in=-200

δ=0.95  #discount rate


##Estimation-1
#The one without unknown parameter in BC
function cut_off(x)
Random.seed!(1234);
dist=Normal(0, x[10]^2)

#Calculate the cut-offs
cut_offs = zeros(N, T_data, K)
Emax = zeros(N, T_data, K)
ini_exp = zeros(N)
k = collect(0:1:K)
for i=1:N
  println("This is person", i)
  ini_exp[i]=A[(i-1)*20+1,4]  #record each individiual's initial experience
  for t=T_data:-1:1
    if t==T_data
      for j=round(Int,ini_exp[i]+1):round(Int,ini_exp[i]+t-1)
        if -x[3]*k[j]-x[1]-x[2]*A[(i-1)*20+t,8]+(1+x[2])*x[5]-x[4]*A[(i-1)*20+t,6]<=0
          cut_offs[i,t,j]=-100
        else
        cut_offs[i,t,j]=log(-x[3]*k[j]-x[1]-x[2]*A[(i-1)*20+t,8]+(1+x[2])*x[5]-x[4]*A[(i-1)*20+t,6])-log(1+x[2])-(x[6]+x[7]*k[j]+x[8]*k[j]*k[j]+x[9]*A[(i-1)*20+t,6])
        end
        #Emax function
        Emax[i,t,j]=(x[1]+(1+x[2])*(A[(i-1)*20+t,8]-x[5])+x[3]*k[j]+x[4]*A[(i-1)*20+t,6])*(1-cdf(dist, cut_offs[i,t,j]))+(1+x[2])*exp(x[6]+x[7]*k[j]+x[8]*k[j]^2+x[9]*A[(i-1)*20+t,6])*exp(0.5*x[10]^2)*(1-cdf(dist,cut_offs[i,t,j]-x[10]^2))+A[(i-1)*20+t,8]*cdf(dist,cut_offs[i,t,j])

      end
    else
      for j=round(Int,ini_exp[i]+1):round(Int,ini_exp[i]+t-1)
        if -x[3]*k[j]-x[1]-x[2]*A[(i-1)*20+t,8]+(1+x[2])*x[5]-x[4]*A[(i-1)*20+t,6]+δ*(Emax[i,t+1,j]-Emax[i,t+1,j+1])<=0
          cut_offs[i,t,j]=-100
        else
        cut_offs[i,t,j]=log(-x[3]*k[j]-x[1]-x[2]*A[(i-1)*20+t,8]+(1+x[2])*x[5]-x[4]*A[(i-1)*20+t,6]+δ*(Emax[i,t+1,j]-Emax[i,t+1,j+1]))-log(1+x[2])-(x[6]+x[7]*k[j]+x[8]*k[j]*k[j]+x[9]*A[(i-1)*20+t,6])
        end
        #Emax function
        Emax[i,t,j]=(x[1]+(1+x[2])*(A[(i-1)*20+t,8]-x[5])+x[3]*k[j]+x[4]*A[(i-1)*20+t,6]+δ*Emax[i,t+1,j+1])*(1-cdf(dist, cut_offs[i,t,j]))+(1+x[2])*exp(x[6]+x[7]*k[j]+x[8]*k[j]*k[j]+x[9]*A[(i-1)*20+t,6])*exp(0.5*x[10]^2)*(1-cdf(dist,cut_offs[i,t,j]-x[10]^2))+(A[(i-1)*20+t,8]+δ*Emax[i,t+1,j])*cdf(dist,cut_offs[i,t,j])
      end
    end
  end
end
return cut_offs
end


x0=[α1_in,α2_in,α3_in,α4_in,b_in,β1_in,β2_in,β3_in,β4_in,σ_in,σe_in]
cut_offs=cut_off(x0)



function llk(x)
  cut_offs=cut_off(x)
  dist=Normal(0, x[10]^2)
  dist_u=Normal(0, x[10]^2+x[11]^2)
  pr=ones(N,10)
  log_pr=zeros(N,10)
  u=zeros(N,10) #u is the sum of wage shock and measurement error
  llk=0
  for i=1:N
    for t=2:10
      if A[(i-1)*20+t,3]==0
        pr[i,t]=cdf(dist,cut_offs[i,t,round(Int,A[(i-1)*20+t-1,4])+1])
      else
        u[i,t]=log(A[(i-1)*20+t,5])-(x[6]+x[7]*A[(i-1)*20+t-1,4]+x[8]*A[(i-1)*20+t-1,4]^2+x[9]*A[(i-1)*20+t,4])
        pr[i,t]=(1-cdf(dist, (cut_offs[i,t,round(Int,A[(i-1)*20+t-1,4])+1]-x[10]^2/(x[10]^2+x[11]^2)*u[i,t])/(x[11]/sqrt(x[10]^2+x[11]^2))))/(sqrt(x[10]^2+x[11]^2))*pdf(dist_u,u[i,t])
        #pr[i,t]=(1-cdf(dist, (cut_offs[i,t,round(Int,A[(i-1)*20+t,4])+1]-(σ^2+σe^2)*u[i,t])/(σ*σe/sqrt(σ^2+σe^2))))/(sqrt(σ^2+σe^2))*pdf(dist_u,u[i,t])
      end
    end
  end
  log_pr=log.(pr)
  llk=sum(log_pr)
  return -llk
end
x0=[α1_in,α2_in,α3_in,α4_in,b_in,β1_in,β2_in,β3_in,β4_in,σ_in,σe_in]
a=llk(x0)

f(x)=llk(x)
x0=[α1_in,α2_in,α3_in,α4_in,b_in,β1_in,β2_in,β3_in,β4_in,σ_in,σe_in]
result=optimize(f, x0 , g_tol=100, iterations=10)
mini_BFGS=Optim.minimizer(result)
print(mini_BFGS)
